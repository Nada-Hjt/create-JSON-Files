{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Finale.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNvS87sOs5VqJypPQurUund",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Nada24/create-JSON-Files/blob/main/Finale.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G4p0PGDD9c24",
        "outputId": "3c871ead-347d-48c0-db5e-499c3e9b2863"
      },
      "source": [
        "!pip3 install python-docx\n",
        "!pip install docx\n",
        "!pip install spacy\n",
        "!python -m spacy download fr_core_news_sm\n",
        "\n",
        "from docx import Document\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import nltk\n",
        "import re\n",
        "import unicodedata\n",
        "import string\n",
        "import fr_core_news_sm\n",
        "import numpy as np\n",
        "import json\n",
        "from nltk.corpus import stopwords\n",
        "nlp = fr_core_news_sm.load()\n",
        "nltk.download('stopwords')\n",
        "\n",
        "def cleaning(test):\n",
        "   test=test.lower()\n",
        "   test = test.translate(str.maketrans('','',string.punctuation))\n",
        "   test=test.strip()\n",
        "   test=''.join( c for c in test if  c not in \"’'–\" )\n",
        "   return test\n",
        "\n",
        "def return_token(sentence):\n",
        "    doc = nlp(sentence)\n",
        "    return [X.text for X in doc]\n",
        "\n",
        "def remove_stopw(test):\n",
        "    stopWords = set(stopwords.words('french'))\n",
        "    clean_words = ''\n",
        "    for token in return_token(test):\n",
        "        if token not in stopWords:\n",
        "            clean_words = clean_words + token + ' '\n",
        "    return clean_words\n",
        "\n",
        "def lemm(test):\n",
        "  tokens = [] \n",
        "  test3 = nlp(test)\n",
        "  for token in test3: \n",
        "     tokens.append(token) \n",
        "  \n",
        "  lemmatized_sentence = \" \".join([token.lemma_ for token in test3]) \n",
        "  return lemmatized_sentence\n",
        "\n",
        "def accentfree(test):\n",
        "  test2 = ''.join((c for c in unicodedata.normalize('NFD', test) if unicodedata.category(c) != 'Mn'))\n",
        "  return test2\n",
        "\n",
        "def normalize(test):\n",
        "  test=cleaning(test)\n",
        "  test=remove_stopw(test)\n",
        "  test=lemm(test)\n",
        "  test = accentfree(test)\n",
        "  test=' '.join(test.split())\n",
        "  return test\n",
        "\n",
        "#************************ Initial Conditions ********************************\n",
        "def Initcond (ch) :   \n",
        "   lista=[]\n",
        "   i=\"Initial Conditions\"\n",
        "   dictt = { i :[]}\n",
        "   listax= ch.splitlines( )\n",
        "   for line in listax:\n",
        "       if re.search(r'«', line):\n",
        "          x = normalize(line[:line.find('«')])\n",
        "          y = accentfree(line[line.find('«')+1:line.rfind('»')].replace('\\u00a0',''))\n",
        "       elif re.search(r'LP', line):\n",
        "          x = normalize(line[:line.find('LP')])\n",
        "          y = line[line.find('LP'):-1].replace('\\u00a0','')\n",
        "       elif re.search(r'Diagbox', line): \n",
        "           x = normalize(line[line.find('Diagbox'):])\n",
        "           y = ''\n",
        "       elif re.search(r'DiagBox', line): \n",
        "           x = normalize(line[line.find('DiagBox'):])\n",
        "           y = ''\n",
        "       elif re.search(r'Ecran affiché', line) :\n",
        "           x = normalize(line[:line.find('choix')]).replace('\\u00a0','')\n",
        "           y = line[line.find('choix'):]\n",
        "       else :\n",
        "         x = normalize(line)\n",
        "         y=''\n",
        "       dict_temp={x:y}\n",
        "       lista.append(dict_temp) \n",
        "\n",
        "   dictt[i]=lista\n",
        "   return dictt\n",
        "\n",
        "#************************ Procedure ********************************\n",
        "\n",
        "def Procedure(ch):\n",
        "   lista=[]\n",
        "   i=\"Procedure Passage\"\n",
        "   dictt = { i :[]}\n",
        "   \n",
        "   \n",
        "   listax= ch.splitlines( )\n",
        "   for line in listax:\n",
        "          if re.search(r'«', line):\n",
        "             x = normalize(line[:line.find('«')])\n",
        "             y = accentfree(line[line.find('«')+1:line.rfind('»')].replace('\\u00a0',''))\n",
        "          elif re.search(r'bouton', line):\n",
        "             x = normalize(line[line.find('bouton'):line.find('bouton')+len('bouton'):])\n",
        "             y = accentfree(line[line.find('bouton')+len('bouton'):-1].replace('\\u00a0',''))\n",
        "          elif re.search(r'Valider', line):\n",
        "             x = normalize(line[line.find('Valider'):line.find('Valider')+len('Valider'):])\n",
        "             y = accentfree(line[line.find('Valider')+len('Valider'):].replace('\\u2019',''))\n",
        "          elif re.search(r'LP', line):\n",
        "             x = normalize(line[:line.find('LP')])\n",
        "             y = accentfree(line[line.find('LP'):-1].replace('\\u00a0',''))\n",
        "          else :\n",
        "            x = normalize(line)\n",
        "            y=''\n",
        "          dict_temp={x:y}\n",
        "          lista.append(dict_temp) \n",
        "          \n",
        "   dictt[i]=lista\n",
        "   return dictt\n",
        "#*********************Predicted results********************************\n",
        "def Results (ch) : \n",
        "   results={} \n",
        "   lista=[]\n",
        "   dictt={}\n",
        "   i=j=o=k=0\n",
        "   m=\"Predicted Results\"\n",
        "   dictt = { m :[]}\n",
        "   listax= ch.splitlines( )\n",
        "   for line in listax :\n",
        "       if re.search(r'«', line) and re.search(r'boutons', line) :\n",
        "          x = normalize(line[:line.find('«')])\n",
        "          y = accentfree(line[line.find('«')+1:line.rfind('»')].replace('\\u00a0',''))  \n",
        "          a = normalize(line[line.find('boutons'):line.find('boutons')+len('boutons'):])\n",
        "          b = accentfree(remove_stopw(line[line.find('boutons')+len('boutons suivants :'):-2].replace(',','')).strip())\n",
        "          c = b.split(' ')\n",
        "          dict_temp={x:y}\n",
        "          dict_temp2={a:c}\n",
        "          lista.append(dict_temp)\n",
        "          lista.append(dict_temp2)\n",
        "       elif len(re.findall(r'«', line))==2 :\n",
        "          k=line[:line.rfind('et')]\n",
        "          x = normalize(k[:k.find('«')])\n",
        "          y = accentfree(k[k.find('«')+1:k.rfind('»')].replace('\\u00a0','')) \n",
        "          s=line[line.rfind('et'):]\n",
        "          a = normalize(s[:s.find('«')])\n",
        "          b = accentfree(s[s.find('«')+1:s.rfind('»')].replace('\\u00a0',''))\n",
        "          dict_temp={x:y}\n",
        "          dict_temp2={a:b}\n",
        "          lista.append(dict_temp)\n",
        "          lista.append(dict_temp2)\n",
        "       elif re.search(r'«', line):\n",
        "          key = normalize(line[:line.find('«')])\n",
        "          results[key] = accentfree(line[line.find('«')+1:line.rfind('»')].replace('\\u00a0',''))\n",
        "       elif re.search(r'Valider', line):\n",
        "          key = normalize(l[l.find('Valider'):l.find('Valider')+len('Valider'):])\n",
        "          results[key] = accentfree(l[l.find('Valider')+len('Valider'):].replace('\\u2019',''))\n",
        "       elif re.search(r'l’écran', line) and re.search(r'choix', line):\n",
        "           key = normalize(line[:line.find('choix')])\n",
        "           results[key] = accentfree(line[line.find('choix'):].replace('\\u2019',''))\n",
        "       else :\n",
        "         key = normalize(line)\n",
        "         results[key]=''\n",
        "   if len(results.keys())>0 :\n",
        "    lista.append(results)\n",
        "   dictt[m]=lista    \n",
        "   return dictt\n",
        "\n",
        "  \n",
        "#********************Prog Principale************************************\n",
        "\n",
        "if __name__ == '__main__':\n",
        "\n",
        " document = Document('/content/input.docx')\n",
        " tables = []\n",
        " columns =['Initial conditions', 'Operating procedure', 'Predicted result', 'R', 'Kx', 'Comments']\n",
        " init_df= pd.DataFrame(columns=columns)\n",
        " for index,table in enumerate(document.tables): #index : numero de table \n",
        "     df = [['' for i in range(len(table.columns))] for j in range(len(table.rows))]\n",
        "     for i, row in enumerate(table.rows):\n",
        "         for j, cell in enumerate(row.cells):       #chaque tableau on le crée un Dataframe\n",
        "             df[i][j] = cell.text\n",
        "     if pd.DataFrame(df).iloc[0][0] == \"Initial conditions\" :  #on garde seulement les tableaux de validation\n",
        "       for E in df: del E[3]\n",
        "       init_df = pd.concat([init_df,pd.DataFrame(df)])\n",
        "\n",
        "\n",
        " pd.DataFrame(init_df).to_excel(\"Table_final.xlsx\",index=False,header=None)\n",
        " dff= pd.read_excel('Table_final.xlsx')\n",
        "\n",
        "\n",
        "# ///////////////////////////////////////////////\n",
        " demo = [\"Test ID : 1601\",\"Test ID : 1608\",\"Test ID : 1611\",\"Test ID : 1623\",\"Test ID : 1625\",\"Test ID : 1857\",\"Test ID : 1660\"]\n",
        " ch=\"Test ID :\"\n",
        " globall= { i :[]}\n",
        " \n",
        " for k in range (len(dff)):\n",
        "   if (ch in dff.iloc[k]['Initial conditions']):   # si on est ds la case d'ID\n",
        "     if (dff.iloc[k]['Initial conditions'] in demo): # Lire uniquement les tests de la liste demo (enlever pour lire tous les ID)\n",
        "       concatt=[]\n",
        "       globall.clear()\n",
        "       i=dff.iloc[k]['Initial conditions']  #Test ID\n",
        "       res1 = Initcond(dff.iloc[k+1]['Initial conditions'])\n",
        "       res2=Procedure(dff.iloc[k+1]['Operating procedure'])  \n",
        "       res3=Results(dff.iloc[k+1]['Predicted result'])\n",
        "       concatt.append(res1)\n",
        "       concatt.append(res2)\n",
        "       concatt.append(res3)\n",
        "       globall[i]=concatt\n",
        "       with open(str(i[10:])+\".json\", 'w') as f:\n",
        "          json.dump(globall, f,indent=True)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        " "
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: python-docx in /usr/local/lib/python3.7/dist-packages (0.8.10)\n",
            "Requirement already satisfied: lxml>=2.3.2 in /usr/local/lib/python3.7/dist-packages (from python-docx) (4.2.6)\n",
            "Requirement already satisfied: docx in /usr/local/lib/python3.7/dist-packages (0.2.4)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.7/dist-packages (from docx) (4.2.6)\n",
            "Requirement already satisfied: Pillow>=2.0 in /usr/local/lib/python3.7/dist-packages (from docx) (7.0.0)\n",
            "Requirement already satisfied: spacy in /usr/local/lib/python3.7/dist-packages (2.2.4)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy) (3.0.5)\n",
            "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.0.5)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.1.3)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.0.5)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.19.5)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (4.41.1)\n",
            "Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (7.4.0)\n",
            "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.0.0)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (0.8.2)\n",
            "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (0.4.1)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (2.23.0)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy) (2.0.5)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy) (54.1.2)\n",
            "Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy) (3.7.2)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2020.12.5)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy) (3.7.4.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy) (3.4.1)\n",
            "Requirement already satisfied: fr_core_news_sm==2.2.5 from https://github.com/explosion/spacy-models/releases/download/fr_core_news_sm-2.2.5/fr_core_news_sm-2.2.5.tar.gz#egg=fr_core_news_sm==2.2.5 in /usr/local/lib/python3.7/dist-packages (2.2.5)\n",
            "Requirement already satisfied: spacy>=2.2.2 in /usr/local/lib/python3.7/dist-packages (from fr_core_news_sm==2.2.5) (2.2.4)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->fr_core_news_sm==2.2.5) (1.1.3)\n",
            "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->fr_core_news_sm==2.2.5) (1.0.0)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->fr_core_news_sm==2.2.5) (3.0.5)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->fr_core_news_sm==2.2.5) (2.0.5)\n",
            "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->fr_core_news_sm==2.2.5) (0.4.1)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->fr_core_news_sm==2.2.5) (2.23.0)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->fr_core_news_sm==2.2.5) (1.0.5)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->fr_core_news_sm==2.2.5) (0.8.2)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->fr_core_news_sm==2.2.5) (1.19.5)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->fr_core_news_sm==2.2.5) (4.41.1)\n",
            "Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->fr_core_news_sm==2.2.5) (7.4.0)\n",
            "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->fr_core_news_sm==2.2.5) (1.0.5)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->fr_core_news_sm==2.2.5) (54.1.2)\n",
            "Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->fr_core_news_sm==2.2.5) (3.7.2)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->fr_core_news_sm==2.2.5) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->fr_core_news_sm==2.2.5) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->fr_core_news_sm==2.2.5) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->fr_core_news_sm==2.2.5) (2020.12.5)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->fr_core_news_sm==2.2.5) (3.7.4.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->fr_core_news_sm==2.2.5) (3.4.1)\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the model via spacy.load('fr_core_news_sm')\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}